{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 09:03:55,042\tINFO resource_spec.py:212 -- Starting Ray with 2.0 GiB memory available for workers and up to 1.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-09 09:03:55,717\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "#ray.init(huge_pages=True, plasma_directory=\"/mnt/hugepages\")\n",
    "ray.init()\n",
    "@ray.remote(num_cpus=1,num_gpus=0)\n",
    "class ParameterServer():\n",
    "\tdef __init__(self):\n",
    "\t\tself.parameters = dict()\n",
    "\t\tself.parameters['q'] = dict()\n",
    "\n",
    "\tdef get(self):\n",
    "\t\treturn self.parameters\n",
    "\n",
    "\tdef put_Q_dict(self, new_q_dict):\n",
    "\t\tself.parameters['q'] = new_q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 09:04:00,931\tWARNING worker.py:1058 -- The actor or task with ID fffffffffffffffff66d17ba0100 is infeasible and cannot currently be scheduled. It requires {GPU: 1.000000}, {CPU: 1.000000} for execution and {CPU: 1.000000}, {GPU: 1.000000} for placement, however there are no nodes in the cluster that can provide the requested resources. To resolve this issue, consider reducing the resource requests of this task or add nodes that can fit the task.\n"
     ]
    }
   ],
   "source": [
    "from actor import Actor\n",
    "from learner import Learner\n",
    "from replay import ReplayMemory\n",
    "\n",
    "num_actors = 3\n",
    "B = 50\n",
    "periodic_update_frequency = 400\n",
    "shared_experience_replay_size = 2 * 10 ** 6\n",
    "num_actions = 6\n",
    "env = 'PongNoFrameskip-v4'\n",
    "\n",
    "shared_memory = ReplayMemory.remote(shared_experience_replay_size, alpha=0.6)\n",
    "shared_state = ParameterServer.remote()\n",
    "\n",
    "learner = Learner.remote(num_actions, shared_memory = shared_memory, shared_state = shared_state)\n",
    "\n",
    "actors = [Actor.remote(i, env, shared_memory, shared_state, n=3) for i in range(num_actors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectID(6170691ebdfaeef644ee453c010000c801000000),\n",
       " ObjectID(cd8f5689d0aa5a397e0a4dfc010000c801000000),\n",
       " ObjectID(55c3b2b635949d816f53dca1010000c801000000)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.run.remote(1000) for a in actors]#actors[0].run.remote(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectID(9fc77bf30b438997f66d17ba010000c801000000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.learn.remote(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(ZeroDivisionError)",
     "evalue": "\u001b[36mray::ReplayMemory.sample()\u001b[39m (pid=720, ip=10.132.128.211)\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n  File \"/Users/gerty/Desktop/DQN/RL/ApeX_DQN/replay.py\", line 343, in sample\n    p_min = self._it_min.min() / self._it_sum.sum()\nZeroDivisionError: float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(ZeroDivisionError)\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-16652e3017ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperience_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#new_priorities, loss = self.train(experience_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids, timeout)\u001b[0m\n\u001b[1;32m   1502\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(ZeroDivisionError)\u001b[0m: \u001b[36mray::ReplayMemory.sample()\u001b[39m (pid=720, ip=10.132.128.211)\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n  File \"/Users/gerty/Desktop/DQN/RL/ApeX_DQN/replay.py\", line 343, in sample\n    p_min = self._it_min.min() / self._it_sum.sum()\nZeroDivisionError: float division by zero"
     ]
    }
   ],
   "source": [
    "experience_batch = ray.get(shared_memory.sample.remote(32, 0.6))\n",
    "#new_priorities, loss = self.train(experience_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "s,a,r,s_prime, gamma, qS_tpn, qS_t, key, weights, idxes = experience_batch\n",
    "#s,a,r,s_prime,done_mask,weights,idxes = experience_batch\n",
    "\n",
    "weights = torch.Tensor(weights).to(device)\n",
    "s = torch.as_tensor(s).to(device)\n",
    "a = torch.LongTensor(a).unsqueeze(1).to(device)\n",
    "r = torch.as_tensor(r).unsqueeze(1).to(device)\n",
    "s_prime = torch.as_tensor(s_prime).to(device)\n",
    "gamma = torch.Tensor(gamma).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Qnet\n",
    "q = Qnet(84,84,4,6).to(device)\n",
    "q_target = Qnet(84,84,4,6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_out = q(s)\n",
    "q_a = q_out.gather(1,a)\n",
    "\n",
    "argmax_q = q(s_prime).argmax(1).unsqueeze(1)\n",
    "q_prime = q_target(s_prime).gather(1,argmax_q)\n",
    "target = r + gamma * q_prime# * done_mask\n",
    "TD_errors = q_a - target\n",
    "with torch.no_grad():\n",
    "    new_priorities = np.abs(TD_errors.cpu()) + 10 ** -5\n",
    "loss = ((TD_errors ** 2).view(-1) * weights).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(shared_memory.sample.remote(2,0.5))\n",
    "ray.get(shared_memory.size.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Qnet\n",
    "net = Qnet(84,84,4,6)\n",
    "net2 = Qnet(84,84,4,6).cuda()\n",
    "data = torch.rand((1,4,84,84))\n",
    "data2 = data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    out = net(data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 09:04:16,062\tWARNING worker.py:1058 -- The actor or task with ID ffffffffffffffff6f53dca10100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:10.132.128.211: 1.000000}, {object_store_memory: 0.683594 GiB}, {memory: 2.001953 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-03-09 09:04:18,770\tERROR worker.py:998 -- Possible unhandled error from worker: \u001b[36mray::Actor.__init__()\u001b[39m (pid=723, ip=10.132.128.211)\n",
      "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/gerty/Desktop/DQN/RL/ApeX_DQN/actor.py\", line 65, in __init__\n",
      "    self.q.load_state_dict(q_weights)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 839, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for Qnet:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\".\n",
      "2020-03-09 09:04:18,772\tERROR worker.py:998 -- Possible unhandled error from worker: \u001b[36mray::Actor.__init__()\u001b[39m (pid=721, ip=10.132.128.211)\n",
      "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/gerty/Desktop/DQN/RL/ApeX_DQN/actor.py\", line 65, in __init__\n",
      "    self.q.load_state_dict(q_weights)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 839, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for Qnet:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\".\n",
      "2020-03-09 09:04:18,773\tERROR worker.py:998 -- Possible unhandled error from worker: \u001b[36mray::Actor.__init__()\u001b[39m (pid=723, ip=10.132.128.211)\n",
      "  File \"python/ray/_raylet.pyx\", line 437, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/gerty/Desktop/DQN/RL/ApeX_DQN/actor.py\", line 65, in __init__\n",
      "    self.q.load_state_dict(q_weights)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 839, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for Qnet:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\".\n",
      "2020-03-09 09:04:18,774\tERROR worker.py:998 -- Possible unhandled error from worker: \u001b[36mray::Actor.__init__()\u001b[39m (pid=721, ip=10.132.128.211)\n",
      "  File \"python/ray/_raylet.pyx\", line 437, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/gerty/Desktop/DQN/RL/ApeX_DQN/actor.py\", line 65, in __init__\n",
      "    self.q.load_state_dict(q_weights)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 839, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for Qnet:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\".\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    out = net2(data2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
